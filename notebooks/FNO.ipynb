{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook designed to create an FNO which matches evolved turbulence simulations to their forcing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralop.training.losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LpLoss, H1Loss\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnamed_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NamedTensorDataset\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trainer2 \n\u001b[1;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/GradSchool/Research/turbulence_encryption/FNO/Training.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m     wandb_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneuralop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmpu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcomm\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LpLoss\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneuralop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer \u001b[38;5;66;03m#load_training_state, save_training_state\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainer2\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralop.training.losses'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import neuralop\n",
    "from neuralop.models import FNO\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.utils import count_model_params\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neuralop.training.losses import LpLoss, H1Loss\n",
    "from named_dataset import NamedTensorDataset\n",
    "from Training import trainer2 \n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_path = \"/work/09694/carsonmcvay/ls6/turbulence_encryption/forcing_functions/multi_forcing_simulations_combined_viscneg2.npz\"\n",
    "data = jnp.load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate inputs and outputs\n",
    "# inputs are simulations, outputs are forcings\n",
    "inputs = data[\"inputs\"]\n",
    "inputs = jnp.fft.irfft(inputs, axis=-1)  # Perform inverse FFT along the last axis # adding this to try to debug\n",
    "outputs = data[\"outputs\"]\n",
    "\n",
    "print(\"Transformed Inputs Shape:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from JAX arrays to NumPy\n",
    "# doing this before passing to PyTorch\n",
    "X_train_np = np.array(jnp.asarray(X_train).block_until_ready())  # Ensures conversion to NumPy\n",
    "y_train_np = np.array(jnp.asarray(y_train).block_until_ready())\n",
    "X_test_np = np.array(jnp.asarray(X_test).block_until_ready())\n",
    "y_test_np = np.array(jnp.asarray(y_test).block_until_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32)\n",
    "\n",
    "# Create data loaders\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_dataset = NamedTensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = NamedTensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# data_processor = data_processor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat FNO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNO(n_modes=(16,16),\n",
    "           in_channels=1,\n",
    "           out_channels=1, \n",
    "           hidden_channels=32,\n",
    "           projection_channel_ratio=2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f\"our model has {n_params} parameters\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr=1e-4,\n",
    "                 weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "# create losses\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr=1e-4,\n",
    "                 weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "# create losses\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer\n",
    "trainer = Trainer(model=model, n_epochs=20, \n",
    "                 wandb_log=False,\n",
    "                 device=device,\n",
    "                 mixed_precision=False,\n",
    "                 data_processor=None,\n",
    "                 eval_interval=3,\n",
    "                 log_output=False,\n",
    "                 use_distributed=False,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on data\n",
    "trainer.train(train_loader=train_loader, \n",
    "             test_loaders={\"default\": test_loader},\n",
    "             optimizer=optimizer,\n",
    "             scheduler=scheduler,\n",
    "             regularizer=False,\n",
    "             training_loss=train_loss,\n",
    "             eval_losses=eval_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
